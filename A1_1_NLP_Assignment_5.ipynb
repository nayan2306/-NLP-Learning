{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA8aKYL0SUPW",
        "outputId": "feb45812-9d26-44f1-d062-767185a801e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URSLcNQqQf-X"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "def learnRETagger(simpleSentence):\n",
        " customPatterns = [\n",
        " (r'.*ing$', 'ADJECTIVE'),\n",
        " (r'.*ly$', 'ADVERB'),\n",
        " (r'.*ion$', 'NOUN'),\n",
        " (r'(.*ate|.*en|is)$', 'VERB'),\n",
        " (r'^an$', 'INDEFINITE-ARTICLE'),\n",
        " (r'^(with|on|at|above|between|)$', 'PREPOSITION'),\n",
        " (r'^(a|an|the)$', 'ARTICLE'),\n",
        " (r'^\\-?[0-9]+(\\.[0-9]+)$', 'NUMBER'),\n",
        " (r'^(i| he| him| you| we| him| her| yours| theirs| someone| where| when| yourselves| themselves| oneself| is| hers| when| whom| whose|)$', 'PRONOUN'),\n",
        " (r'.*$', None),\n",
        " ]\n",
        " tagger = nltk.RegexpTagger(customPatterns)\n",
        " wordsInSentence = nltk.word_tokenize(simpleSentence)\n",
        " posEnabledTags = tagger.tag(wordsInSentence)\n",
        " print(posEnabledTags)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = input('Enter any sentence')\n",
        "sent = sentence.lower()\n",
        "learnRETagger(sent )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GiErrylSYFg",
        "outputId": "c55bd2fe-83a5-4c51-cb87-4ed6ecc5a078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter any sentencei love chess\n",
            "[('i', 'PRONOUN'), ('love', None), ('chess', None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#chunking using the customized grammar\n",
        "import nltk\n",
        "\n",
        "# Define the customized grammar\n",
        "grammar = r\"\"\"\n",
        "    NP: {<DT>?<JJ>*<NN>}  # Noun phrase\n",
        "    ADJP: {<RB>?<JJ>+}   # Adjective phrase\n",
        "    ADVP: {<RB>+}        # Adverb phrase\n",
        "    VP: {<VB.*><DT|RB|RP|TO>?<JJ>*<PRP.*|NN.*>+}  # Verb phrase\n",
        "\"\"\"\n",
        "\n",
        "# Create the chunk parser using the customized grammar\n",
        "chunk_parser = nltk.RegexpParser(grammar)\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Tag the parts-of-speech of the tokens\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Parse the tagged sentence using the customized grammar\n",
        "tree = chunk_parser.parse(pos_tags)\n",
        "\n",
        "# Print the resulting tree\n",
        "print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcagAQs8ddxy",
        "outputId": "0c098d73-4271-4039-bfbf-24c26bbae44f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP The/DT quick/JJ brown/NN)\n",
            "  (NP fox/NN)\n",
            "  jumps/VBZ\n",
            "  over/IN\n",
            "  (NP the/DT lazy/JJ dog/NN))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#chunking using inbuilt functions\n",
        "# Define the input sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Tag the parts-of-speech of the tokens\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Use the inbuilt function to perform chunking\n",
        "tree = nltk.ne_chunk(pos_tags)\n",
        "\n",
        "# Print the resulting tree\n",
        "print(tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqvoKnNfdy4S",
        "outputId": "78e0f72f-8eec-4331-d448-76c42fc87995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  The/DT\n",
            "  quick/JJ\n",
            "  brown/NN\n",
            "  fox/NN\n",
            "  jumps/VBZ\n",
            "  over/IN\n",
            "  the/DT\n",
            "  lazy/JJ\n",
            "  dog/NN)\n"
          ]
        }
      ]
    }
  ]
}